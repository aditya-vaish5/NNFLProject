{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nnfl_termproject_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-vaish5/NNFLProject/blob/master/nnfl_termproject_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaNEhwReywhz",
        "colab_type": "text"
      },
      "source": [
        "Import train and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uer1xgJvH6L6",
        "colab_type": "code",
        "outputId": "f7f2ff51-18ad-4a80-b70f-706e2b38841f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-25 17:14:49--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13603614 (13M) [text/plain]\n",
            "Saving to: ‘train.en’\n",
            "\n",
            "train.en            100%[===================>]  12.97M  3.62MB/s    in 3.6s    \n",
            "\n",
            "2020-05-25 17:14:53 (3.62 MB/s) - ‘train.en’ saved [13603614/13603614]\n",
            "\n",
            "--2020-05-25 17:14:55--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18074646 (17M) [text/plain]\n",
            "Saving to: ‘train.vi’\n",
            "\n",
            "train.vi            100%[===================>]  17.24M  4.25MB/s    in 4.1s    \n",
            "\n",
            "2020-05-25 17:15:00 (4.25 MB/s) - ‘train.vi’ saved [18074646/18074646]\n",
            "\n",
            "--2020-05-25 17:15:02--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 132264 (129K) [text/plain]\n",
            "Saving to: ‘tst2013.en’\n",
            "\n",
            "tst2013.en          100%[===================>] 129.16K   157KB/s    in 0.8s    \n",
            "\n",
            "2020-05-25 17:15:03 (157 KB/s) - ‘tst2013.en’ saved [132264/132264]\n",
            "\n",
            "--2020-05-25 17:15:06--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 183855 (180K) [text/plain]\n",
            "Saving to: ‘tst2013.vi’\n",
            "\n",
            "tst2013.vi          100%[===================>] 179.55K   219KB/s    in 0.8s    \n",
            "\n",
            "2020-05-25 17:15:07 (219 KB/s) - ‘tst2013.vi’ saved [183855/183855]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ7M0vJvy9qC",
        "colab_type": "text"
      },
      "source": [
        "importing librarires numpy , PyTorch , Pandas , regex , unicodedata and time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advDQCZZIi60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLlDZa2wzWJd",
        "colab_type": "text"
      },
      "source": [
        "Saving train and test data in Lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlaQsvAUO9Jm",
        "colab_type": "code",
        "outputId": "071125a5-d62d-427f-9f04-29d5e4056e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 943
        }
      },
      "source": [
        "source_sent = []\n",
        "target_sent = []\n",
        "\n",
        "test_source_sent = []\n",
        "test_target_sent = []\n",
        "\n",
        "\n",
        "with open('train.en', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        # discarding first 20 translations as there was some\n",
        "        # english to english translations found in the first few. which are wrong\n",
        "        if l_i<50:\n",
        "            continue\n",
        "        source_sent.append(line)\n",
        "        \n",
        "            \n",
        "with open('train.vi', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        if l_i<50:\n",
        "            continue\n",
        "        target_sent.append(line)\n",
        "\n",
        "\n",
        "with open('tst2013.en', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        test_source_sent.append(line)\n",
        "                    \n",
        "with open('tst2013.vi', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        test_target_sent.append(line)\n",
        "            \n",
        "assert len(source_sent) == len(target_sent),'Source: %d, Target: %d'%(len(source_sent),len(target_sent))\n",
        "assert len(test_source_sent) == len(test_target_sent), 'Source: %d, Target: %d'%(len(test_source_sent), len(test_target_sent))\n",
        "print('Sample translations (%d)'%len(source_sent))\n",
        "for i in range(0,len(source_sent),10000):\n",
        "    print('(',i,') EN: ', source_sent[i])\n",
        "    print('(',i,') VI: ', target_sent[i])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample translations (133267)\n",
            "( 0 ) EN:  In each one of those assessments that we write , we always tag on a summary , and the summary is written for a non-scientific audience .\n",
            "\n",
            "( 0 ) VI:  Trong mỗi bản đánh giá chúng tôi viết , chúng tôi luôn đính kèm một bản tóm lược , được viết cho những độc giả không chuyên về khoa học .\n",
            "\n",
            "( 10000 ) EN:  This is an area in the prefrontal cortex , a region where we can use cognition to try to overcome aversive emotional states .\n",
            "\n",
            "( 10000 ) VI:  Đây là một khu vực trong vỏ não trước trán , vùng mà chúng sử dụng tri thức cho việc thử vượt qua trạng thái cảm xúc ác cảm .\n",
            "\n",
            "( 20000 ) EN:  And there are flowers that are self-infertile . That means they can &apos;t -- the pollen in their bloom can &apos;t fertilize themselves .\n",
            "\n",
            "( 20000 ) VI:  có những loài hoa không thể tự thụ phấn . Nghĩa là chúng không thể -- phấn hoa của nó không thể tụ thụ phấn được\n",
            "\n",
            "( 30000 ) EN:  And a lot of this comes together in a philosophy of change that I find really is powerful .\n",
            "\n",
            "( 30000 ) VI:  Và nhiều như vậy hợp lại thành một triết lý của sự thay đổi mà tôi thấy là thực sự rất mạnh .\n",
            "\n",
            "( 40000 ) EN:  Dean Ornish : At first for a long time , I wrote messages in notebooks .\n",
            "\n",
            "( 40000 ) VI:  Dean Ornish : &quot; Trong một khoảng thời gian dài ban đầu , tôi đã viết các tin nhắn trên các cuốn ghi chú .\n",
            "\n",
            "( 50000 ) EN:  World &apos;s first bamboo bike with folding handlebars .\n",
            "\n",
            "( 50000 ) VI:  Chiếc xe đạp bằng tre đầu tiên trên thế giới với ghi đông gập .\n",
            "\n",
            "( 60000 ) EN:  We need to invest more resources into research and treatment of mental illness .\n",
            "\n",
            "( 60000 ) VI:  Chúng ta cần đầu tư nhiều nguồn lực hơn cho công cuộc nghiên cứu và chữa trị về bệnh thần kinh .\n",
            "\n",
            "( 70000 ) EN:  If we are providing knowledge and experience , we need to structure that .\n",
            "\n",
            "( 70000 ) VI:  Nếu chúng ta cung cấp kiến thức và kinh nghiệm , chúng ta cần cơ cấu nó .\n",
            "\n",
            "( 80000 ) EN:  But I say it has to be under the conditions I &apos;ve always worked : no credit , no logos , no sponsoring .\n",
            "\n",
            "( 80000 ) VI:  Nhưng tôi nói nó phải theo các điều kiện tôi luôn luôn làm không có tín dụng , không có biểu tượng , không có tài trợ .\n",
            "\n",
            "( 90000 ) EN:  What would it look like ?\n",
            "\n",
            "( 90000 ) VI:  Nó sẽ trông như thế nào ?\n",
            "\n",
            "( 100000 ) EN:  And the 70 year-old ones , actually they &apos;re better at scouting out the good nesting places , and they also have more progeny every year .\n",
            "\n",
            "( 100000 ) VI:  Và những con 70 tuổi , thực sự giỏi hơn trong việc tìm kiếm một nơi để dựng tổ , và chúng cũng có nhiều con hơn hàng năm\n",
            "\n",
            "( 110000 ) EN:  The next time you dine on sushi -- or sashimi , or swordfish steak , or shrimp cocktail , whatever wildlife you happen to enjoy from the ocean -- think of the real cost .\n",
            "\n",
            "( 110000 ) VI:  Khi bạn thưởng thức sushi , hay sashimi , hay thịt cá kiếm nướng , hay cốc-tai tôm , bất kể thứ gì hoang dã từ đại dương mà bạn thưởng thức , hãy nghĩ về cái giá thực sự phải trả .\n",
            "\n",
            "( 120000 ) EN:  When I laid out my plan , I realized that I faced three main challenges : first , creating a sensor ; second , designing a circuit ; and third , coding a smartphone app .\n",
            "\n",
            "( 120000 ) VI:  Khi lập kế hoạch , tôi nhận ra mình đối mặt với 3 thách thức : thứ nhất , tạo ra một cảm biến ; thứ hai , thiết kế bảng mạch ; thứ ba , lập trình ứng dụng .\n",
            "\n",
            "( 130000 ) EN:  Why would you do something that dangerous ?\n",
            "\n",
            "( 130000 ) VI:  Tại sao bạn lại sẵn sàng làm một việc nguy hiểm như thế ?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd5ixidIzdcZ",
        "colab_type": "text"
      },
      "source": [
        "Importing pretrained glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sua1PaHGabby",
        "colab_type": "code",
        "outputId": "bd099ebb-6df0-42b1-d2d0-9e81405fae4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-25 17:08:17--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-25 17:08:17--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-25 17:08:18--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.94MB/s    in 6m 29s  \n",
            "\n",
            "2020-05-25 17:14:47 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azVXeHjOzotS",
        "colab_type": "text"
      },
      "source": [
        "Import gzip for unzipping glove embeddings in runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLpl2TsqnSH9",
        "colab_type": "code",
        "outputId": "7d7057d1-8c90-451f-ac45-ceb633163b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "!apt install gzip\n",
        "import gzip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "gzip is already the newest version (1.6-5ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpI7R8ShzvcQ",
        "colab_type": "text"
      },
      "source": [
        "Current directory structure in Runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZBi6z6uMsVD",
        "colab_type": "code",
        "outputId": "ccf7929c-b28d-4291-a43c-982ab0d7084d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip  train.en  tst2013.en\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data   train.vi  tst2013.vi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTU3RZTCz664",
        "colab_type": "text"
      },
      "source": [
        "installing and importing libraries to read glove embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-rFbKdODWLS",
        "colab_type": "code",
        "outputId": "60ddfdc8-4686-4a24-bdb2-b2a8e8c6af1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!pip install bcolz"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from bcolz) (1.18.4)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp36-cp36m-linux_x86_64.whl size=2656151 sha256=eb7dbca078b55a9bcd37f3b3d87a7766b43c8a97c501d54cbe3a95320baa161c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n",
            "Installing collected packages: bcolz\n",
            "Successfully installed bcolz-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc5OWKT9DkUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bcolz\n",
        "import pickle\n",
        "import copy\n",
        "import operator\n",
        "from pandas import DataFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLu8_LyV0DVe",
        "colab_type": "text"
      },
      "source": [
        "Saving glove embeddings and their values in 2 files for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOcxhfJUDoCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = bcolz.carray(np.zeros(1), rootdir='/content/6B.200.dat', mode='w')\n",
        "with open('/content/glove.6B.200d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors.append(vect)\n",
        "     \n",
        "vectors = bcolz.carray(vectors[1:].reshape((400000, 200)), rootdir='/content/6B.200.dat', mode='w')\n",
        "vectors.flush()\n",
        "pickle.dump(words, open('/content/6B.200_words.pkl', 'wb'))\n",
        "pickle.dump(word2idx, open('/content/6B.200_idx.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTx41pHp0XCp",
        "colab_type": "text"
      },
      "source": [
        "Creating a dictionary glove with words mapping to their embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm13VvTBDriu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = bcolz.open('/content/6B.200.dat')[:]\n",
        "words = pickle.load(open('/content/6B.200_words.pkl', 'rb'))\n",
        "word2idx = pickle.load(open('/content/6B.200_idx.pkl', 'rb'))\n",
        "\n",
        "glove = {w: vectors[word2idx[w]] for w in words}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onrGZM-K0gbx",
        "colab_type": "text"
      },
      "source": [
        "Displaying glove embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkvfUMMADx9t",
        "colab_type": "code",
        "outputId": "74a8756b-9127-464a-fc53-72bc482b9b3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "glove_dframe = DataFrame(vectors, columns=range(1,201), index=words)\n",
        "print(glove_dframe.shape)\n",
        "glove_dframe[0:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400000, 200)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.071549</td>\n",
              "      <td>0.093459</td>\n",
              "      <td>0.023738</td>\n",
              "      <td>-0.090339</td>\n",
              "      <td>0.056123</td>\n",
              "      <td>0.32547</td>\n",
              "      <td>-0.397960</td>\n",
              "      <td>-0.092139</td>\n",
              "      <td>0.061181</td>\n",
              "      <td>-0.189500</td>\n",
              "      <td>0.130610</td>\n",
              "      <td>0.14349</td>\n",
              "      <td>0.011479</td>\n",
              "      <td>0.381580</td>\n",
              "      <td>0.540300</td>\n",
              "      <td>-0.140880</td>\n",
              "      <td>0.243150</td>\n",
              "      <td>0.230360</td>\n",
              "      <td>-0.553390</td>\n",
              "      <td>0.048154</td>\n",
              "      <td>0.456620</td>\n",
              "      <td>3.2338</td>\n",
              "      <td>0.020199</td>\n",
              "      <td>0.049019</td>\n",
              "      <td>-0.014132</td>\n",
              "      <td>0.076017</td>\n",
              "      <td>-0.115270</td>\n",
              "      <td>0.200600</td>\n",
              "      <td>-0.077657</td>\n",
              "      <td>0.243280</td>\n",
              "      <td>0.163680</td>\n",
              "      <td>-0.341180</td>\n",
              "      <td>-0.066070</td>\n",
              "      <td>0.101520</td>\n",
              "      <td>0.038232</td>\n",
              "      <td>-0.176680</td>\n",
              "      <td>-0.88153</td>\n",
              "      <td>-0.33895</td>\n",
              "      <td>-0.035481</td>\n",
              "      <td>-0.550950</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.042910</td>\n",
              "      <td>-0.067897</td>\n",
              "      <td>-0.293320</td>\n",
              "      <td>0.109780</td>\n",
              "      <td>-0.045365</td>\n",
              "      <td>0.232220</td>\n",
              "      <td>-0.311340</td>\n",
              "      <td>-0.289830</td>\n",
              "      <td>-0.666870</td>\n",
              "      <td>0.53097</td>\n",
              "      <td>0.194610</td>\n",
              "      <td>0.366700</td>\n",
              "      <td>0.26185</td>\n",
              "      <td>-0.651870</td>\n",
              "      <td>0.102660</td>\n",
              "      <td>0.113630</td>\n",
              "      <td>-0.129530</td>\n",
              "      <td>-0.682460</td>\n",
              "      <td>-0.187510</td>\n",
              "      <td>0.147600</td>\n",
              "      <td>1.07650</td>\n",
              "      <td>-0.229080</td>\n",
              "      <td>-0.009343</td>\n",
              "      <td>-0.206510</td>\n",
              "      <td>-0.352250</td>\n",
              "      <td>-0.267200</td>\n",
              "      <td>-0.003431</td>\n",
              "      <td>0.25906</td>\n",
              "      <td>0.217590</td>\n",
              "      <td>0.661580</td>\n",
              "      <td>0.121800</td>\n",
              "      <td>0.199570</td>\n",
              "      <td>-0.20303</td>\n",
              "      <td>0.344740</td>\n",
              "      <td>-0.243280</td>\n",
              "      <td>0.131390</td>\n",
              "      <td>-0.008877</td>\n",
              "      <td>0.336170</td>\n",
              "      <td>0.030591</td>\n",
              "      <td>0.255770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>,</th>\n",
              "      <td>0.176510</td>\n",
              "      <td>0.292080</td>\n",
              "      <td>-0.002077</td>\n",
              "      <td>-0.375230</td>\n",
              "      <td>0.004914</td>\n",
              "      <td>0.23979</td>\n",
              "      <td>-0.288930</td>\n",
              "      <td>-0.014643</td>\n",
              "      <td>-0.109930</td>\n",
              "      <td>0.155920</td>\n",
              "      <td>0.206270</td>\n",
              "      <td>0.47675</td>\n",
              "      <td>0.099907</td>\n",
              "      <td>-0.140580</td>\n",
              "      <td>0.211140</td>\n",
              "      <td>0.121260</td>\n",
              "      <td>-0.318310</td>\n",
              "      <td>-0.089433</td>\n",
              "      <td>-0.090553</td>\n",
              "      <td>-0.319620</td>\n",
              "      <td>0.213190</td>\n",
              "      <td>2.4844</td>\n",
              "      <td>-0.077521</td>\n",
              "      <td>-0.084279</td>\n",
              "      <td>0.201860</td>\n",
              "      <td>0.260840</td>\n",
              "      <td>-0.404110</td>\n",
              "      <td>-0.191270</td>\n",
              "      <td>0.247150</td>\n",
              "      <td>0.223940</td>\n",
              "      <td>-0.063437</td>\n",
              "      <td>0.203790</td>\n",
              "      <td>-0.184630</td>\n",
              "      <td>-0.088413</td>\n",
              "      <td>0.024169</td>\n",
              "      <td>-0.287690</td>\n",
              "      <td>-0.61246</td>\n",
              "      <td>-0.12683</td>\n",
              "      <td>-0.088273</td>\n",
              "      <td>0.183310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026823</td>\n",
              "      <td>-0.045444</td>\n",
              "      <td>-0.226420</td>\n",
              "      <td>-0.199770</td>\n",
              "      <td>-0.121380</td>\n",
              "      <td>0.169410</td>\n",
              "      <td>0.061998</td>\n",
              "      <td>0.426310</td>\n",
              "      <td>-0.088383</td>\n",
              "      <td>0.45756</td>\n",
              "      <td>0.077774</td>\n",
              "      <td>0.061342</td>\n",
              "      <td>0.45710</td>\n",
              "      <td>-0.177870</td>\n",
              "      <td>-0.145970</td>\n",
              "      <td>0.326540</td>\n",
              "      <td>0.002443</td>\n",
              "      <td>-0.118860</td>\n",
              "      <td>0.100810</td>\n",
              "      <td>-0.020011</td>\n",
              "      <td>1.03660</td>\n",
              "      <td>-0.398140</td>\n",
              "      <td>-0.681800</td>\n",
              "      <td>0.236850</td>\n",
              "      <td>-0.203960</td>\n",
              "      <td>-0.176680</td>\n",
              "      <td>-0.313850</td>\n",
              "      <td>0.14834</td>\n",
              "      <td>-0.052187</td>\n",
              "      <td>0.061300</td>\n",
              "      <td>-0.325820</td>\n",
              "      <td>0.191530</td>\n",
              "      <td>-0.15469</td>\n",
              "      <td>-0.146790</td>\n",
              "      <td>0.046971</td>\n",
              "      <td>0.032325</td>\n",
              "      <td>-0.220060</td>\n",
              "      <td>-0.207740</td>\n",
              "      <td>-0.231890</td>\n",
              "      <td>-0.108140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0.122890</td>\n",
              "      <td>0.580370</td>\n",
              "      <td>-0.069635</td>\n",
              "      <td>-0.502880</td>\n",
              "      <td>0.105030</td>\n",
              "      <td>0.39945</td>\n",
              "      <td>-0.386350</td>\n",
              "      <td>-0.084279</td>\n",
              "      <td>0.122190</td>\n",
              "      <td>0.080312</td>\n",
              "      <td>0.323370</td>\n",
              "      <td>0.47579</td>\n",
              "      <td>-0.038375</td>\n",
              "      <td>-0.007090</td>\n",
              "      <td>0.415240</td>\n",
              "      <td>0.321210</td>\n",
              "      <td>-0.211850</td>\n",
              "      <td>0.361440</td>\n",
              "      <td>-0.055623</td>\n",
              "      <td>-0.030512</td>\n",
              "      <td>0.428540</td>\n",
              "      <td>2.8547</td>\n",
              "      <td>-0.146230</td>\n",
              "      <td>-0.175570</td>\n",
              "      <td>0.311970</td>\n",
              "      <td>-0.131180</td>\n",
              "      <td>0.033298</td>\n",
              "      <td>0.130930</td>\n",
              "      <td>0.089889</td>\n",
              "      <td>-0.124170</td>\n",
              "      <td>0.002340</td>\n",
              "      <td>-0.068954</td>\n",
              "      <td>-0.107540</td>\n",
              "      <td>-0.115510</td>\n",
              "      <td>-0.310520</td>\n",
              "      <td>-0.120970</td>\n",
              "      <td>-0.46691</td>\n",
              "      <td>-0.08360</td>\n",
              "      <td>-0.037664</td>\n",
              "      <td>-0.071779</td>\n",
              "      <td>...</td>\n",
              "      <td>0.075441</td>\n",
              "      <td>0.082116</td>\n",
              "      <td>-0.460080</td>\n",
              "      <td>0.012393</td>\n",
              "      <td>-0.025310</td>\n",
              "      <td>0.141770</td>\n",
              "      <td>-0.092192</td>\n",
              "      <td>0.345050</td>\n",
              "      <td>-0.521360</td>\n",
              "      <td>0.57304</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>0.033196</td>\n",
              "      <td>0.29672</td>\n",
              "      <td>-0.278990</td>\n",
              "      <td>0.199790</td>\n",
              "      <td>0.256660</td>\n",
              "      <td>0.082079</td>\n",
              "      <td>-0.078436</td>\n",
              "      <td>0.093719</td>\n",
              "      <td>0.242020</td>\n",
              "      <td>1.34950</td>\n",
              "      <td>-0.304340</td>\n",
              "      <td>-0.309360</td>\n",
              "      <td>0.420470</td>\n",
              "      <td>-0.079068</td>\n",
              "      <td>-0.148190</td>\n",
              "      <td>-0.089404</td>\n",
              "      <td>0.06680</td>\n",
              "      <td>0.224050</td>\n",
              "      <td>0.272260</td>\n",
              "      <td>-0.035236</td>\n",
              "      <td>0.176880</td>\n",
              "      <td>-0.05360</td>\n",
              "      <td>0.007003</td>\n",
              "      <td>-0.033006</td>\n",
              "      <td>-0.080021</td>\n",
              "      <td>-0.244510</td>\n",
              "      <td>-0.039174</td>\n",
              "      <td>-0.162360</td>\n",
              "      <td>-0.096652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.052924</td>\n",
              "      <td>0.254270</td>\n",
              "      <td>0.313530</td>\n",
              "      <td>-0.356130</td>\n",
              "      <td>0.029629</td>\n",
              "      <td>0.51034</td>\n",
              "      <td>-0.107160</td>\n",
              "      <td>0.151950</td>\n",
              "      <td>0.057698</td>\n",
              "      <td>0.061490</td>\n",
              "      <td>0.061160</td>\n",
              "      <td>0.39911</td>\n",
              "      <td>-0.000290</td>\n",
              "      <td>0.319780</td>\n",
              "      <td>0.432570</td>\n",
              "      <td>-0.147080</td>\n",
              "      <td>0.054842</td>\n",
              "      <td>0.270790</td>\n",
              "      <td>-0.140510</td>\n",
              "      <td>-0.301010</td>\n",
              "      <td>0.163130</td>\n",
              "      <td>3.0013</td>\n",
              "      <td>0.222310</td>\n",
              "      <td>-0.142790</td>\n",
              "      <td>0.083705</td>\n",
              "      <td>0.089866</td>\n",
              "      <td>-0.527060</td>\n",
              "      <td>-0.089661</td>\n",
              "      <td>0.273110</td>\n",
              "      <td>0.314130</td>\n",
              "      <td>-0.040810</td>\n",
              "      <td>0.060557</td>\n",
              "      <td>-0.042656</td>\n",
              "      <td>0.241780</td>\n",
              "      <td>-0.291870</td>\n",
              "      <td>0.225750</td>\n",
              "      <td>-0.62980</td>\n",
              "      <td>-0.14641</td>\n",
              "      <td>-0.224290</td>\n",
              "      <td>-0.056621</td>\n",
              "      <td>...</td>\n",
              "      <td>0.405320</td>\n",
              "      <td>-0.027960</td>\n",
              "      <td>-0.133980</td>\n",
              "      <td>-0.110860</td>\n",
              "      <td>0.059506</td>\n",
              "      <td>0.240520</td>\n",
              "      <td>-0.597390</td>\n",
              "      <td>-0.002407</td>\n",
              "      <td>-0.185930</td>\n",
              "      <td>1.04200</td>\n",
              "      <td>-0.129690</td>\n",
              "      <td>0.208130</td>\n",
              "      <td>0.33305</td>\n",
              "      <td>-0.127800</td>\n",
              "      <td>0.085662</td>\n",
              "      <td>-0.076422</td>\n",
              "      <td>0.314070</td>\n",
              "      <td>-0.237840</td>\n",
              "      <td>-0.054838</td>\n",
              "      <td>0.011369</td>\n",
              "      <td>0.84500</td>\n",
              "      <td>-0.341650</td>\n",
              "      <td>0.093983</td>\n",
              "      <td>0.082445</td>\n",
              "      <td>-0.277770</td>\n",
              "      <td>-0.442260</td>\n",
              "      <td>-0.063078</td>\n",
              "      <td>0.37274</td>\n",
              "      <td>0.054468</td>\n",
              "      <td>0.241970</td>\n",
              "      <td>-0.040886</td>\n",
              "      <td>0.389400</td>\n",
              "      <td>-0.10509</td>\n",
              "      <td>0.233720</td>\n",
              "      <td>0.096027</td>\n",
              "      <td>-0.303240</td>\n",
              "      <td>0.244880</td>\n",
              "      <td>-0.086254</td>\n",
              "      <td>-0.419170</td>\n",
              "      <td>0.464960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.573460</td>\n",
              "      <td>0.541700</td>\n",
              "      <td>-0.234770</td>\n",
              "      <td>-0.362400</td>\n",
              "      <td>0.403700</td>\n",
              "      <td>0.11386</td>\n",
              "      <td>-0.449330</td>\n",
              "      <td>-0.309910</td>\n",
              "      <td>-0.005341</td>\n",
              "      <td>0.584260</td>\n",
              "      <td>-0.025956</td>\n",
              "      <td>0.49393</td>\n",
              "      <td>-0.037209</td>\n",
              "      <td>-0.284280</td>\n",
              "      <td>0.097696</td>\n",
              "      <td>-0.489070</td>\n",
              "      <td>0.026027</td>\n",
              "      <td>0.376490</td>\n",
              "      <td>0.057788</td>\n",
              "      <td>-0.468070</td>\n",
              "      <td>0.081288</td>\n",
              "      <td>3.2825</td>\n",
              "      <td>-0.636900</td>\n",
              "      <td>0.379560</td>\n",
              "      <td>0.003817</td>\n",
              "      <td>0.093607</td>\n",
              "      <td>-0.128550</td>\n",
              "      <td>0.173800</td>\n",
              "      <td>0.105220</td>\n",
              "      <td>0.286480</td>\n",
              "      <td>0.210890</td>\n",
              "      <td>-0.470760</td>\n",
              "      <td>0.027733</td>\n",
              "      <td>-0.198030</td>\n",
              "      <td>0.076328</td>\n",
              "      <td>-0.846290</td>\n",
              "      <td>-0.79708</td>\n",
              "      <td>-0.38743</td>\n",
              "      <td>-0.030422</td>\n",
              "      <td>-0.268490</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.124130</td>\n",
              "      <td>-0.344310</td>\n",
              "      <td>-0.232960</td>\n",
              "      <td>-0.211870</td>\n",
              "      <td>0.085387</td>\n",
              "      <td>0.070063</td>\n",
              "      <td>-0.198030</td>\n",
              "      <td>-0.026023</td>\n",
              "      <td>-0.390370</td>\n",
              "      <td>0.80002</td>\n",
              "      <td>0.405770</td>\n",
              "      <td>-0.079863</td>\n",
              "      <td>0.35263</td>\n",
              "      <td>-0.340430</td>\n",
              "      <td>0.396760</td>\n",
              "      <td>0.228620</td>\n",
              "      <td>-0.350280</td>\n",
              "      <td>-0.473440</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>-0.116570</td>\n",
              "      <td>1.05520</td>\n",
              "      <td>-0.415700</td>\n",
              "      <td>-0.080552</td>\n",
              "      <td>-0.056571</td>\n",
              "      <td>-0.166220</td>\n",
              "      <td>0.192740</td>\n",
              "      <td>-0.095175</td>\n",
              "      <td>-0.20781</td>\n",
              "      <td>0.156200</td>\n",
              "      <td>0.050231</td>\n",
              "      <td>-0.279150</td>\n",
              "      <td>0.437420</td>\n",
              "      <td>-0.31237</td>\n",
              "      <td>0.131940</td>\n",
              "      <td>-0.332780</td>\n",
              "      <td>0.188770</td>\n",
              "      <td>-0.234220</td>\n",
              "      <td>0.544180</td>\n",
              "      <td>-0.230690</td>\n",
              "      <td>0.349470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>0.203270</td>\n",
              "      <td>0.473480</td>\n",
              "      <td>0.050877</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.060547</td>\n",
              "      <td>0.33066</td>\n",
              "      <td>0.048486</td>\n",
              "      <td>0.021504</td>\n",
              "      <td>-0.536310</td>\n",
              "      <td>0.213120</td>\n",
              "      <td>0.199830</td>\n",
              "      <td>0.51408</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.094641</td>\n",
              "      <td>0.068724</td>\n",
              "      <td>0.274240</td>\n",
              "      <td>-0.204930</td>\n",
              "      <td>0.232680</td>\n",
              "      <td>0.324900</td>\n",
              "      <td>-0.194440</td>\n",
              "      <td>0.646930</td>\n",
              "      <td>2.8342</td>\n",
              "      <td>0.140040</td>\n",
              "      <td>-0.268680</td>\n",
              "      <td>0.273250</td>\n",
              "      <td>0.015312</td>\n",
              "      <td>-0.279750</td>\n",
              "      <td>-0.264230</td>\n",
              "      <td>0.141830</td>\n",
              "      <td>-0.026064</td>\n",
              "      <td>0.113490</td>\n",
              "      <td>0.250390</td>\n",
              "      <td>-0.249720</td>\n",
              "      <td>-0.168820</td>\n",
              "      <td>-0.310390</td>\n",
              "      <td>-0.444580</td>\n",
              "      <td>-0.34789</td>\n",
              "      <td>-0.20181</td>\n",
              "      <td>-0.013405</td>\n",
              "      <td>0.236350</td>\n",
              "      <td>...</td>\n",
              "      <td>0.573390</td>\n",
              "      <td>0.180440</td>\n",
              "      <td>-0.117810</td>\n",
              "      <td>0.351620</td>\n",
              "      <td>0.162200</td>\n",
              "      <td>0.554830</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.232100</td>\n",
              "      <td>-0.202050</td>\n",
              "      <td>0.60227</td>\n",
              "      <td>-0.153790</td>\n",
              "      <td>0.219070</td>\n",
              "      <td>0.28405</td>\n",
              "      <td>0.011906</td>\n",
              "      <td>0.106220</td>\n",
              "      <td>0.506700</td>\n",
              "      <td>-0.432010</td>\n",
              "      <td>-0.408870</td>\n",
              "      <td>-0.178190</td>\n",
              "      <td>0.220420</td>\n",
              "      <td>1.07750</td>\n",
              "      <td>-0.393810</td>\n",
              "      <td>-0.358280</td>\n",
              "      <td>0.363020</td>\n",
              "      <td>0.148720</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>-0.030339</td>\n",
              "      <td>-0.11273</td>\n",
              "      <td>0.023382</td>\n",
              "      <td>0.159040</td>\n",
              "      <td>-0.143890</td>\n",
              "      <td>-0.117540</td>\n",
              "      <td>-0.63655</td>\n",
              "      <td>-0.121970</td>\n",
              "      <td>0.043809</td>\n",
              "      <td>0.147160</td>\n",
              "      <td>0.073750</td>\n",
              "      <td>-0.213580</td>\n",
              "      <td>-0.622490</td>\n",
              "      <td>0.143860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>-0.102720</td>\n",
              "      <td>0.304100</td>\n",
              "      <td>-0.135770</td>\n",
              "      <td>-0.279790</td>\n",
              "      <td>-0.409260</td>\n",
              "      <td>-0.26553</td>\n",
              "      <td>0.104920</td>\n",
              "      <td>-0.044101</td>\n",
              "      <td>0.062731</td>\n",
              "      <td>-0.041600</td>\n",
              "      <td>0.355880</td>\n",
              "      <td>0.40757</td>\n",
              "      <td>-0.142950</td>\n",
              "      <td>-0.036534</td>\n",
              "      <td>0.425120</td>\n",
              "      <td>0.014823</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.291500</td>\n",
              "      <td>-0.204160</td>\n",
              "      <td>-0.100390</td>\n",
              "      <td>0.307670</td>\n",
              "      <td>3.1815</td>\n",
              "      <td>0.045614</td>\n",
              "      <td>0.094457</td>\n",
              "      <td>0.255450</td>\n",
              "      <td>0.275280</td>\n",
              "      <td>-0.299390</td>\n",
              "      <td>0.045123</td>\n",
              "      <td>0.446810</td>\n",
              "      <td>0.015012</td>\n",
              "      <td>-0.107850</td>\n",
              "      <td>-0.389880</td>\n",
              "      <td>-0.205620</td>\n",
              "      <td>0.263060</td>\n",
              "      <td>-0.018163</td>\n",
              "      <td>-0.142440</td>\n",
              "      <td>-0.56610</td>\n",
              "      <td>-0.12168</td>\n",
              "      <td>0.287490</td>\n",
              "      <td>-0.305810</td>\n",
              "      <td>...</td>\n",
              "      <td>0.208250</td>\n",
              "      <td>-0.171630</td>\n",
              "      <td>-0.320250</td>\n",
              "      <td>-0.275370</td>\n",
              "      <td>-0.307740</td>\n",
              "      <td>0.181840</td>\n",
              "      <td>-0.026260</td>\n",
              "      <td>-0.062733</td>\n",
              "      <td>-0.436660</td>\n",
              "      <td>0.57954</td>\n",
              "      <td>-0.323480</td>\n",
              "      <td>-0.134570</td>\n",
              "      <td>0.38565</td>\n",
              "      <td>-0.301980</td>\n",
              "      <td>0.261970</td>\n",
              "      <td>-0.100340</td>\n",
              "      <td>-0.146000</td>\n",
              "      <td>-0.341700</td>\n",
              "      <td>0.166710</td>\n",
              "      <td>-0.235990</td>\n",
              "      <td>1.24670</td>\n",
              "      <td>-0.005984</td>\n",
              "      <td>-0.569670</td>\n",
              "      <td>0.526400</td>\n",
              "      <td>-0.210240</td>\n",
              "      <td>-0.298610</td>\n",
              "      <td>-0.293000</td>\n",
              "      <td>0.15889</td>\n",
              "      <td>0.172540</td>\n",
              "      <td>-0.002398</td>\n",
              "      <td>0.071749</td>\n",
              "      <td>0.053166</td>\n",
              "      <td>-0.23429</td>\n",
              "      <td>-0.084927</td>\n",
              "      <td>0.155390</td>\n",
              "      <td>0.418200</td>\n",
              "      <td>-0.152160</td>\n",
              "      <td>0.369510</td>\n",
              "      <td>0.190390</td>\n",
              "      <td>-0.122660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>0.241690</td>\n",
              "      <td>-0.345340</td>\n",
              "      <td>-0.223070</td>\n",
              "      <td>-1.290700</td>\n",
              "      <td>0.252850</td>\n",
              "      <td>-0.55128</td>\n",
              "      <td>-0.080336</td>\n",
              "      <td>-0.008177</td>\n",
              "      <td>0.311360</td>\n",
              "      <td>-0.451010</td>\n",
              "      <td>0.246610</td>\n",
              "      <td>0.36441</td>\n",
              "      <td>0.943360</td>\n",
              "      <td>-0.035420</td>\n",
              "      <td>0.780480</td>\n",
              "      <td>-0.397650</td>\n",
              "      <td>0.311250</td>\n",
              "      <td>-0.177430</td>\n",
              "      <td>-0.419890</td>\n",
              "      <td>-0.378150</td>\n",
              "      <td>0.672300</td>\n",
              "      <td>3.1716</td>\n",
              "      <td>0.032496</td>\n",
              "      <td>-0.031640</td>\n",
              "      <td>0.580680</td>\n",
              "      <td>-0.444580</td>\n",
              "      <td>-0.055612</td>\n",
              "      <td>0.180520</td>\n",
              "      <td>0.285720</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.214370</td>\n",
              "      <td>0.049731</td>\n",
              "      <td>0.187200</td>\n",
              "      <td>0.119140</td>\n",
              "      <td>0.027408</td>\n",
              "      <td>-0.806080</td>\n",
              "      <td>-0.30835</td>\n",
              "      <td>-0.89737</td>\n",
              "      <td>-0.197720</td>\n",
              "      <td>0.026741</td>\n",
              "      <td>...</td>\n",
              "      <td>0.092912</td>\n",
              "      <td>-0.060809</td>\n",
              "      <td>0.029073</td>\n",
              "      <td>-0.387350</td>\n",
              "      <td>-0.070853</td>\n",
              "      <td>-0.659750</td>\n",
              "      <td>-0.381570</td>\n",
              "      <td>0.501700</td>\n",
              "      <td>-0.735600</td>\n",
              "      <td>0.41521</td>\n",
              "      <td>0.213280</td>\n",
              "      <td>-0.337790</td>\n",
              "      <td>0.66902</td>\n",
              "      <td>0.424860</td>\n",
              "      <td>-0.121480</td>\n",
              "      <td>-0.010626</td>\n",
              "      <td>0.127450</td>\n",
              "      <td>-0.135610</td>\n",
              "      <td>0.234230</td>\n",
              "      <td>0.351100</td>\n",
              "      <td>1.28410</td>\n",
              "      <td>0.129820</td>\n",
              "      <td>0.213570</td>\n",
              "      <td>0.328570</td>\n",
              "      <td>0.165670</td>\n",
              "      <td>-0.214580</td>\n",
              "      <td>-0.442750</td>\n",
              "      <td>0.32850</td>\n",
              "      <td>0.180010</td>\n",
              "      <td>0.064865</td>\n",
              "      <td>-0.358800</td>\n",
              "      <td>-0.014226</td>\n",
              "      <td>0.31125</td>\n",
              "      <td>-0.220490</td>\n",
              "      <td>0.032829</td>\n",
              "      <td>0.385250</td>\n",
              "      <td>-0.105120</td>\n",
              "      <td>0.278010</td>\n",
              "      <td>-0.101710</td>\n",
              "      <td>-0.071521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"</th>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.312010</td>\n",
              "      <td>-0.597680</td>\n",
              "      <td>-0.125830</td>\n",
              "      <td>-0.275240</td>\n",
              "      <td>0.29145</td>\n",
              "      <td>-0.304310</td>\n",
              "      <td>0.037122</td>\n",
              "      <td>0.944680</td>\n",
              "      <td>0.088085</td>\n",
              "      <td>-0.096273</td>\n",
              "      <td>0.40542</td>\n",
              "      <td>-0.652400</td>\n",
              "      <td>0.377160</td>\n",
              "      <td>0.530010</td>\n",
              "      <td>-0.308190</td>\n",
              "      <td>-0.274780</td>\n",
              "      <td>0.810410</td>\n",
              "      <td>0.536350</td>\n",
              "      <td>0.087590</td>\n",
              "      <td>0.352880</td>\n",
              "      <td>2.8857</td>\n",
              "      <td>-0.125050</td>\n",
              "      <td>-0.035968</td>\n",
              "      <td>0.135500</td>\n",
              "      <td>-0.299320</td>\n",
              "      <td>0.561540</td>\n",
              "      <td>-0.594290</td>\n",
              "      <td>-0.349910</td>\n",
              "      <td>0.685590</td>\n",
              "      <td>-0.115370</td>\n",
              "      <td>0.088894</td>\n",
              "      <td>-0.298290</td>\n",
              "      <td>-0.207680</td>\n",
              "      <td>0.691100</td>\n",
              "      <td>0.068548</td>\n",
              "      <td>-0.50814</td>\n",
              "      <td>-0.97722</td>\n",
              "      <td>0.035782</td>\n",
              "      <td>-0.540530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.197520</td>\n",
              "      <td>-0.254360</td>\n",
              "      <td>0.166160</td>\n",
              "      <td>-0.181350</td>\n",
              "      <td>0.163740</td>\n",
              "      <td>-0.161940</td>\n",
              "      <td>-0.320130</td>\n",
              "      <td>-0.331110</td>\n",
              "      <td>0.536620</td>\n",
              "      <td>0.53301</td>\n",
              "      <td>-0.321660</td>\n",
              "      <td>-0.643620</td>\n",
              "      <td>1.05760</td>\n",
              "      <td>-0.180190</td>\n",
              "      <td>-0.409320</td>\n",
              "      <td>-0.160840</td>\n",
              "      <td>-1.009400</td>\n",
              "      <td>0.229720</td>\n",
              "      <td>0.543100</td>\n",
              "      <td>0.058910</td>\n",
              "      <td>1.75020</td>\n",
              "      <td>-0.197530</td>\n",
              "      <td>-0.054010</td>\n",
              "      <td>0.016083</td>\n",
              "      <td>-0.555230</td>\n",
              "      <td>-0.202310</td>\n",
              "      <td>-0.326130</td>\n",
              "      <td>-0.38783</td>\n",
              "      <td>0.614280</td>\n",
              "      <td>-0.112160</td>\n",
              "      <td>0.423190</td>\n",
              "      <td>-0.447290</td>\n",
              "      <td>-0.35638</td>\n",
              "      <td>-0.326980</td>\n",
              "      <td>-0.126620</td>\n",
              "      <td>-0.288580</td>\n",
              "      <td>0.080920</td>\n",
              "      <td>0.144930</td>\n",
              "      <td>0.052563</td>\n",
              "      <td>0.750070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>'s</th>\n",
              "      <td>-0.005961</td>\n",
              "      <td>0.451480</td>\n",
              "      <td>0.004549</td>\n",
              "      <td>0.020727</td>\n",
              "      <td>0.538770</td>\n",
              "      <td>0.49453</td>\n",
              "      <td>-0.353690</td>\n",
              "      <td>-0.056286</td>\n",
              "      <td>0.057851</td>\n",
              "      <td>-0.204500</td>\n",
              "      <td>-0.306400</td>\n",
              "      <td>0.37904</td>\n",
              "      <td>0.263880</td>\n",
              "      <td>0.360360</td>\n",
              "      <td>0.687150</td>\n",
              "      <td>0.116290</td>\n",
              "      <td>-0.570050</td>\n",
              "      <td>0.084364</td>\n",
              "      <td>-0.671840</td>\n",
              "      <td>0.217060</td>\n",
              "      <td>0.600840</td>\n",
              "      <td>3.1461</td>\n",
              "      <td>0.218390</td>\n",
              "      <td>0.188300</td>\n",
              "      <td>-0.011606</td>\n",
              "      <td>0.702800</td>\n",
              "      <td>-0.054734</td>\n",
              "      <td>-0.576020</td>\n",
              "      <td>-0.505610</td>\n",
              "      <td>0.132750</td>\n",
              "      <td>-0.373490</td>\n",
              "      <td>-0.223150</td>\n",
              "      <td>0.189340</td>\n",
              "      <td>-0.388350</td>\n",
              "      <td>-0.446160</td>\n",
              "      <td>0.250780</td>\n",
              "      <td>-0.24067</td>\n",
              "      <td>-0.27303</td>\n",
              "      <td>-0.635870</td>\n",
              "      <td>0.071991</td>\n",
              "      <td>...</td>\n",
              "      <td>0.664030</td>\n",
              "      <td>-0.196290</td>\n",
              "      <td>-0.066911</td>\n",
              "      <td>0.208120</td>\n",
              "      <td>0.293660</td>\n",
              "      <td>0.330200</td>\n",
              "      <td>-0.219090</td>\n",
              "      <td>-0.279870</td>\n",
              "      <td>0.110260</td>\n",
              "      <td>0.47563</td>\n",
              "      <td>0.497420</td>\n",
              "      <td>0.071567</td>\n",
              "      <td>0.19419</td>\n",
              "      <td>-0.136880</td>\n",
              "      <td>-0.069569</td>\n",
              "      <td>0.497010</td>\n",
              "      <td>-0.843270</td>\n",
              "      <td>-0.447570</td>\n",
              "      <td>0.502530</td>\n",
              "      <td>0.605960</td>\n",
              "      <td>0.86543</td>\n",
              "      <td>0.475590</td>\n",
              "      <td>-0.009351</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>-0.599520</td>\n",
              "      <td>-0.123170</td>\n",
              "      <td>-0.303610</td>\n",
              "      <td>0.17132</td>\n",
              "      <td>0.823690</td>\n",
              "      <td>0.267900</td>\n",
              "      <td>0.334550</td>\n",
              "      <td>0.355210</td>\n",
              "      <td>-0.56247</td>\n",
              "      <td>0.372890</td>\n",
              "      <td>0.515540</td>\n",
              "      <td>-0.003640</td>\n",
              "      <td>0.076358</td>\n",
              "      <td>0.399470</td>\n",
              "      <td>0.296210</td>\n",
              "      <td>0.053627</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          1         2         3    ...       198       199       200\n",
              "the -0.071549  0.093459  0.023738  ...  0.336170  0.030591  0.255770\n",
              ",    0.176510  0.292080 -0.002077  ... -0.207740 -0.231890 -0.108140\n",
              ".    0.122890  0.580370 -0.069635  ... -0.039174 -0.162360 -0.096652\n",
              "of   0.052924  0.254270  0.313530  ... -0.086254 -0.419170  0.464960\n",
              "to   0.573460  0.541700 -0.234770  ...  0.544180 -0.230690  0.349470\n",
              "and  0.203270  0.473480  0.050877  ... -0.213580 -0.622490  0.143860\n",
              "in  -0.102720  0.304100 -0.135770  ...  0.369510  0.190390 -0.122660\n",
              "a    0.241690 -0.345340 -0.223070  ...  0.278010 -0.101710 -0.071521\n",
              "\"    0.001032  0.312010 -0.597680  ...  0.144930  0.052563  0.750070\n",
              "'s  -0.005961  0.451480  0.004549  ...  0.399470  0.296210  0.053627\n",
              "\n",
              "[10 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9qscJ8-08St",
        "colab_type": "text"
      },
      "source": [
        "Moving 'sos' token at index 0 and 'eos' token at index 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPbNIt2WD7Nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sos_index = word2idx['sos']\n",
        "eos_index = word2idx['eos']\n",
        "sos_swap_word = words[0]\n",
        "eos_swap_word = words[1]\n",
        " \n",
        "words[0], words[sos_index] = words[sos_index], words[0]\n",
        "words[1], words[eos_index] = words[eos_index], words[1]\n",
        "word2idx[sos_swap_word], word2idx['sos'] = word2idx['sos'], word2idx[sos_swap_word]\n",
        "word2idx[eos_swap_word], word2idx['eos'] = word2idx['eos'], word2idx[eos_swap_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X2ewJ1e1EVO",
        "colab_type": "text"
      },
      "source": [
        "Sorting words in an dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8cjzqomEBoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort Word2idx\n",
        "word2idx = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdurhxl91Rwt",
        "colab_type": "text"
      },
      "source": [
        "import matplotlib for plotting graphs and visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaUn80e2BZXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rZDxdLL5klY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uH_VuWD5lvI",
        "colab_type": "code",
        "outputId": "44fceae6-ee5d-4867-a473-e978137aa01f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKy_e_CY1Z-I",
        "colab_type": "text"
      },
      "source": [
        "A language class to store all words mapped to an index in the dataset. In this class we also assign all the words in pretrained glove embedding while initializing the language class for english."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhDp726q5pWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        if(name == 'eng'):\n",
        "          self.word2index = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}\n",
        "          self.word2index[\"unk\"] = 400000 \n",
        "          self.word2count = { word : 1 for word in words }\n",
        "          self.index2word = { i : word for word, i in word2idx.items() }\n",
        "          self.n_words = 400001\n",
        "        else:\n",
        "          self.word2index = {}\n",
        "          self.word2count = {}\n",
        "          self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "          self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN5qoqo6114J",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing of sentences to remove all non letter characters and converting sentence to Ascii characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1tAZFIx5sAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zr3tfeV2EYQ",
        "colab_type": "text"
      },
      "source": [
        "Filtering data with sentences having a  maximum length of 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLMVraBNFfmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 100\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH \n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ0MQcxv2PCl",
        "colab_type": "text"
      },
      "source": [
        "Preparing data for for both input and output languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br-e3-7E6EoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(lang1, lang2, source_arr, target_arr):\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "    pairs = [];\n",
        "    for i in range(0,len(target_arr)):\n",
        "      pairs.append([normalizeString(source_arr[i]), normalizeString(target_arr[i])])\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQPFVaa62XSg",
        "colab_type": "text"
      },
      "source": [
        "Preparing data and removing sentences with word length > 100,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rJpzUK52MeJ",
        "colab_type": "code",
        "outputId": "7c0991d1-833f-41d9-e5cc-f9e0a82063bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "input_lang, output_lang, pairs_train = prepareData('eng', 'vi', source_sent, target_sent)\n",
        "print(random.choice(pairs_train))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 133267 sentence pairs\n",
            "Trimmed to 132670 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 402460\n",
            "vi 14127\n",
            "['that was the dogma .', ' o a thanh tin ieu .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL4MzDwh2gu_",
        "colab_type": "text"
      },
      "source": [
        "Defining a matrix with glove embedding of size 200 to initialize in Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upwI_rayXo4e",
        "colab_type": "code",
        "outputId": "b5cb5b13-c31e-4bef-8cc0-f456d7ee135a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# create a new tensor with all embeddings for english using glove dictionary\n",
        "matrix_len = input_lang.n_words\n",
        " \n",
        "weights_matrix = np.zeros((matrix_len, 200))\n",
        "words_found = 0\n",
        "print(input_lang.n_words)\n",
        "print(len(input_lang.word2index))\n",
        "for i, word in enumerate(input_lang.word2index):\n",
        "    try: \n",
        "        weights_matrix[i-2] = glove[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i-2] = np.random.normal(scale=0.6, size=(200, ))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "402460\n",
            "402459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZpdeTsi2vBf",
        "colab_type": "text"
      },
      "source": [
        "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLyqU3WF700M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.input_size = input_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional = True)\n",
        "\n",
        "    def forward(self, input, hidden, cell_state):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output, (hidden, cell_state) = self.lstm(output, (hidden, cell_state))\n",
        "        return output, hidden, cell_state\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(2, 1, self.hidden_size, device = device)\n",
        "    def initcellstate(self):\n",
        "      return torch.zeros(2, 1, self.hidden_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6N7SPIa3ARs",
        "colab_type": "text"
      },
      "source": [
        "The decoder is another RNN that takes the encoder output vector(s) and outputs a sequence of words to create the translation. This class defined a simple decoder without attention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ9UdlQE759q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# THIS CLASS ISN'T USED IN EVALUATION\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size,bidirectional =True)\n",
        "        self.out = nn.Linear(hidden_size*2, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden,cell_state):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden,cell_state = self.lstm(output, (hidden,cell_state))\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden,cell_state\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "    def initcellstate(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VooFq3NF3J6q",
        "colab_type": "text"
      },
      "source": [
        "**Attention decoder**\n",
        "\n",
        "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PotAA8uQ7_aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 100\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size,bidirectional =True)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, cell_state, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        # print(attn_weights.unsqueeze(0).size(), encoder_outputs.unsqueeze(0).size())\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "        # print(embedded[0].size(),attn_applied[0].size())\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, (hidden, cell_state) = self.lstm(output, (hidden, cell_state))\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, cell_state, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(2, 1, self.hidden_size, device = device)\n",
        "    def initcellstate(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5IX-y9x3TNk",
        "colab_type": "text"
      },
      "source": [
        "Preparing input tensors and appending **EOS** End of Sentence token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjVvbet88DNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    arr = [];\n",
        "    for word in sentence.split(' '):\n",
        "        try:\n",
        "          arr.append(lang.word2index[word])\n",
        "        except:\n",
        "          arr.append(lang.word2index[\"unk\"])\n",
        "    return arr\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcd1aE023iS0",
        "colab_type": "text"
      },
      "source": [
        "Traning the model for 1 iteration.\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track of every output and the latest hidden state. Then the decoder is given the <SOS> token as its first input, and the last hidden state of the encoder as its first hidden state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZThP3EA8HRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_cell = encoder.initcellstate()\n",
        "    # print(\"setting optimizers to zero grad\")\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # print(\"INPUT SIZE: \", input_tensor.size())\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "      # print(\"starting encoder for \" , ei)\n",
        "      encoder_output, encoder_hidden, encoder_cell = encoder(\n",
        "          input_tensor[ei], encoder_hidden, encoder_cell)\n",
        "      # print(encoder_output)\n",
        "      encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "    for di in range(target_length):\n",
        "      # print(\"decoder for di : \", di)\n",
        "      decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(\n",
        "          decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "      decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "      loss += criterion(decoder_output, target_tensor[di])\n",
        "      if decoder_input.item() == EOS_token:\n",
        "          break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn2Gamqd8JiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper functions\n",
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueqh7sZ-35di",
        "colab_type": "text"
      },
      "source": [
        "Creating training pairs and calling train function for multiple iterations.\n",
        "\n",
        "Start a timer\n",
        "\n",
        "Initialize optimizers and criterion\n",
        "\n",
        "Create set of training pairs\n",
        "\n",
        "Start empty losses array for plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdSWSxdv8NF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, pairs, print_every=1000, plot_every=100, learning_rate=0.1):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SDY5Z2G8g5L",
        "colab_type": "code",
        "outputId": "ee2efbea-5b8b-4b10-c9ad-257944818e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "hidden_size = 200\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "print(\"Encoder initialization done\")\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "print(\"Decoder initialization done\")\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 1000, pairs_train, print_every=50)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder initialization done\n",
            "Decoder initialization done\n",
            "0m 5s (- 1m 43s) (50 5%) 4.2786\n",
            "0m 11s (- 1m 39s) (100 10%) 3.7148\n",
            "0m 15s (- 1m 28s) (150 15%) 2.8182\n",
            "0m 20s (- 1m 21s) (200 20%) 3.4495\n",
            "0m 26s (- 1m 19s) (250 25%) 2.8874\n",
            "0m 30s (- 1m 11s) (300 30%) 2.2843\n",
            "0m 34s (- 1m 4s) (350 35%) 2.8103\n",
            "0m 38s (- 0m 58s) (400 40%) 2.5427\n",
            "0m 43s (- 0m 52s) (450 45%) 2.7583\n",
            "0m 47s (- 0m 47s) (500 50%) 2.4947\n",
            "0m 52s (- 0m 42s) (550 55%) 2.7759\n",
            "0m 56s (- 0m 37s) (600 60%) 2.1928\n",
            "1m 0s (- 0m 32s) (650 65%) 2.9212\n",
            "1m 6s (- 0m 28s) (700 70%) 3.3000\n",
            "1m 10s (- 0m 23s) (750 75%) 2.5192\n",
            "1m 14s (- 0m 18s) (800 80%) 3.0483\n",
            "1m 19s (- 0m 14s) (850 85%) 2.9457\n",
            "1m 24s (- 0m 9s) (900 90%) 2.8222\n",
            "1m 29s (- 0m 4s) (950 95%) 3.4956\n",
            "1m 34s (- 0m 0s) (1000 100%) 2.6518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R63XjRT_4wnd",
        "colab_type": "text"
      },
      "source": [
        "Preparing test data to evaluate our trained network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-J0HQm5hxK8",
        "colab_type": "code",
        "outputId": "7d7db15c-c427-43a6-e1a4-77e06f7406aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "input_lang_test, output_lang_test, pairs_test = prepareData('eng', 'vi', test_source_sent, test_target_sent)\n",
        "print(random.choice(pairs_test))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 1268 sentence pairs\n",
            "Trimmed to 1265 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 400032\n",
            "vi 1104\n",
            "['so what we can do is use steel wool just to clean pans and the steel wool we can cut in very small pieces and these very small pieces we can mix to the bitumen .', 'nhung gi chung toi co the lam la dung mieng bui nhui bang thep danh e chui rua xoong . va chung toi cat vun mieng bui nhui thep ra that nho va chung toi tron cac manh vun nho nay voi nhua bitum .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdqS_bwG5Fpc",
        "colab_type": "text"
      },
      "source": [
        "Evalutate fuction runs on encoder then feed the output to decoder. And then turs decoded output into words again for later display."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9We9ZU8bre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_cell = encoder.initcellstate()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden,encoder_cell = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden,encoder_cell)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell = encoder_cell\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden,decoder_cell, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1,:ei + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nopoTSKG5dq2",
        "colab_type": "text"
      },
      "source": [
        "Picks a random set form test data and call evaluate() on it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy1dYNPb8eHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "65b05b3a-a024-4ff8-bafc-751be05fde64"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    input=[]\n",
        "    reference=[]\n",
        "    output=[]\n",
        "    for i in range(n):\n",
        "        \n",
        "        pair = random.choice(pairs_test)\n",
        "        print('>', pair[0])\n",
        "        input.append(pair[0])\n",
        "        reference.append(pair[1])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        output.append(output_sentence)\n",
        "        print('')\n",
        "    return output,reference,input\n",
        "\n",
        "\n",
        "\n",
        "output,reference,input=evaluateRandomly(encoder1, attn_decoder1,5)\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> we had these magnificent tomatoes . in italy a tomato would grow to this size . in zambia to this size .\n",
            "= chung toi co nhung qua ca chua rat tuyet voi . tai y mot qua ca chua chi to khoang bang nay . tai zambia no to toi co nay nay .\n",
            "< va o co co la toi o co co co co co co co toi o co co co toi o co toi o co co co co co co toi o co co toi o co co toi o co co co co toi o co toi o co co co co toi o co co co co co co co toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the co ban co ban co <EOS>\n",
            "\n",
            "> we can also act as a bridge between the people inside north korea and the outside world because many of us stay in contact with family members still inside and we send information and money that is helping to change north korea from inside .\n",
            "= chung toi cung co the ong vai tro nhu cau noi giua nhung nguoi ang o trong bac trieu tien voi the gioi ben ngoai boi vi co rat nhieu nguoi trong chung toi ang giu lien lac voi nhung thanh vien gia inh khac o trong nuoc va chung toi chia se voi ho thong tin va tien bac e co the thay oi bac trieu tien tu phia trong .\n",
            "< va co co co la toi o co toi o co co co co co co co co toi o co co co co toi o co co toi o co co co co toi o co co co co co co co co toi o co co co toi o co co co co co toi o co co co co co toi o co co toi o co co toi o co co co co co co co co co co co toi o co co co co co toi o co co toi o co co co\n",
            "\n",
            "> and i am on this stage because i am a model .\n",
            "= va toi ung tren san khau nay boi vi toi la mot nguoi mau .\n",
            "< va co co la toi o co co toi o co co co co co co toi o co co co co toi o co co co toi o co co co co co toi o co co ban co ban co ban co <EOS>\n",
            "\n",
            "> also i realized there was a wide gap between north and south .\n",
            "= toi cung nhan ra mot khoang cach rat lon giua nguoi nam va bac trieu tien .\n",
            "< va o co co la toi o co toi o co co toi o co co co co co co co toi o co co co co co co co co co toi o co co toi o co ban co ban co ban co ban co ban co co ban co ban co ban co ban co co ban co ban co ban co <EOS>\n",
            "\n",
            "> and first i commend you on your model knowledge . very impressive .\n",
            "= toi muon khen ngoi su am hieu ve nguoi mau cua cac ban . rat la an tuong .\n",
            "< va co co co la toi o co co co co co co co toi o co toi o co co co co co co co co co toi o co co co co co co toi o co co co toi o co co co ban co ban co ban co co ban co toi <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DXfF12H8rNL",
        "colab_type": "code",
        "outputId": "a98d08b4-6299-48bb-db79-6beaf9d04c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> kofi was rescued from a fishing village .\n",
            "= kofi uoc cuu tu lang chai .\n",
            "< va co co co la toi o co toi o co co co co toi o co co co toi o co ban co ban co ban co ban co <EOS>\n",
            "\n",
            "> thank you .\n",
            "= vo tay cam on\n",
            "< va co co la toi o co co co co co co co co co co co toi the toi the toi the toi the toi the toi the toi the toi the toi the co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co ban co <EOS>\n",
            "\n",
            "> many have been tricked by false promises of a good education a better job only to find that they apos re forced to work without pay under the threat of violence and they cannot walk away .\n",
            "= rat nhieu nguoi bi lua boi nhung loi hua ieu ngoa ve giao duc tot cong viec tot hon chi e thay minh bi bat lam viec khong cong duoi ach bao luc va khong thoat ra uoc .\n",
            "< va co co co la toi o co toi o co co toi o co co co co toi o co co co toi o co co co co co co co co toi o co toi o co co toi o co co co co co co co co co co co toi o co toi o co co co co toi o co toi o co co co toi o co co co co co co co toi o co toi o co co co toi o co co co co co co co co toi o co\n",
            "\n",
            "> and first i commend you on your model knowledge . very impressive .\n",
            "= toi muon khen ngoi su am hieu ve nguoi mau cua cac ban . rat la an tuong .\n",
            "< va o co co la toi o co co co co co co co co co co co co co toi o co co toi o co co co co toi o co co toi o co co co toi o co ban co ban co ban co ban co ban co co ban co ban co ban co co ban co ban co ban co ban co <EOS>\n",
            "\n",
            ">  quot before i die i want to straddle the international date line . quot \n",
            "=  quot truoc khi toi chet toi muon uoc dang chan tren uong oi ngay quoc te . quot \n",
            "< va co co co la toi o co co co co co co co co co toi o co co co co co co co co co co toi o co co co co co toi o co co co co co co co co toi o co co toi o co co toi the co ban co ban co ban co ban co co ban co <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhRJiydl-5L9",
        "colab_type": "text"
      },
      "source": [
        "Computing BLEU score using nltk library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX3x7aPq-u_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "def calc_bleu(reference,nput,output):\n",
        " scores=[]\n",
        " score=0\n",
        " print(score)\n",
        " for sentence in range(len(reference)):\n",
        "   score = sentence_bleu(reference[sentence],output[sentence])\n",
        "   scores.append(score)\n",
        "  \n",
        " \n",
        " for sentence in range(len(reference)):\n",
        "   print(\"input sentence: \",nput[sentence])\n",
        "   print(\"reference sentence:\",reference[sentence])\n",
        "   print(\"output sentence:\",output[sentence])\n",
        "   print(\"BLEU SCORE:\",scores[sentence])\n",
        "\n",
        " return pd.DataFrame({\"input sentence: \":nput,\"reference sentence:\":reference,\"output sentence:\":output,\"BLEU SCORE:\":scores})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPe9IPTa_AQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "460c48e7-7f48-4795-c4ee-6ea173c1fe45"
      },
      "source": [
        "calc_bleu(reference,input,output)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "input sentence:  we had these magnificent tomatoes . in italy a tomato would grow to this size . in zambia to this size .\n",
            "reference sentence: chung toi co nhung qua ca chua rat tuyet voi . tai y mot qua ca chua chi to khoang bang nay . tai zambia no to toi co nay nay .\n",
            "output sentence: va o co co la toi o co co co co co co co toi o co co co toi o co toi o co co co co co co toi o co co toi o co co toi o co co co co toi o co toi o co co co co toi o co co co co co co co toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the toi the co ban co ban co <EOS>\n",
            "BLEU SCORE: 0.4282636687818112\n",
            "input sentence:  we can also act as a bridge between the people inside north korea and the outside world because many of us stay in contact with family members still inside and we send information and money that is helping to change north korea from inside .\n",
            "reference sentence: chung toi cung co the ong vai tro nhu cau noi giua nhung nguoi ang o trong bac trieu tien voi the gioi ben ngoai boi vi co rat nhieu nguoi trong chung toi ang giu lien lac voi nhung thanh vien gia inh khac o trong nuoc va chung toi chia se voi ho thong tin va tien bac e co the thay oi bac trieu tien tu phia trong .\n",
            "output sentence: va co co co la toi o co toi o co co co co co co co co toi o co co co co toi o co co toi o co co co co toi o co co co co co co co co toi o co co co toi o co co co co co toi o co co co co co toi o co co toi o co co toi o co co co co co co co co co co co toi o co co co co co toi o co co toi o co co co\n",
            "BLEU SCORE: 0.4044405568460044\n",
            "input sentence:  and i am on this stage because i am a model .\n",
            "reference sentence: va toi ung tren san khau nay boi vi toi la mot nguoi mau .\n",
            "output sentence: va co co la toi o co co toi o co co co co co co toi o co co co co toi o co co co toi o co co co co co toi o co co ban co ban co ban co <EOS>\n",
            "BLEU SCORE: 0.5035337887555859\n",
            "input sentence:  also i realized there was a wide gap between north and south .\n",
            "reference sentence: toi cung nhan ra mot khoang cach rat lon giua nguoi nam va bac trieu tien .\n",
            "output sentence: va o co co la toi o co toi o co co toi o co co co co co co co toi o co co co co co co co co co toi o co co toi o co ban co ban co ban co ban co ban co co ban co ban co ban co ban co co ban co ban co ban co <EOS>\n",
            "BLEU SCORE: 0.46658351030877\n",
            "input sentence:  and first i commend you on your model knowledge . very impressive .\n",
            "reference sentence: toi muon khen ngoi su am hieu ve nguoi mau cua cac ban . rat la an tuong .\n",
            "output sentence: va co co co la toi o co co co co co co co toi o co toi o co co co co co co co co co toi o co co co co co co toi o co co co toi o co co co ban co ban co ban co co ban co toi <EOS>\n",
            "BLEU SCORE: 0.4868498039128583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input sentence:</th>\n",
              "      <th>reference sentence:</th>\n",
              "      <th>output sentence:</th>\n",
              "      <th>BLEU SCORE:</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>we had these magnificent tomatoes . in italy a...</td>\n",
              "      <td>chung toi co nhung qua ca chua rat tuyet voi ....</td>\n",
              "      <td>va o co co la toi o co co co co co co co toi o...</td>\n",
              "      <td>0.428264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>we can also act as a bridge between the people...</td>\n",
              "      <td>chung toi cung co the ong vai tro nhu cau noi ...</td>\n",
              "      <td>va co co co la toi o co toi o co co co co co c...</td>\n",
              "      <td>0.404441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>and i am on this stage because i am a model .</td>\n",
              "      <td>va toi ung tren san khau nay boi vi toi la mot...</td>\n",
              "      <td>va co co la toi o co co toi o co co co co co c...</td>\n",
              "      <td>0.503534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>also i realized there was a wide gap between n...</td>\n",
              "      <td>toi cung nhan ra mot khoang cach rat lon giua ...</td>\n",
              "      <td>va o co co la toi o co toi o co co toi o co co...</td>\n",
              "      <td>0.466584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>and first i commend you on your model knowledg...</td>\n",
              "      <td>toi muon khen ngoi su am hieu ve nguoi mau cua...</td>\n",
              "      <td>va co co co la toi o co co co co co co co toi ...</td>\n",
              "      <td>0.486850</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    input sentence:   ... BLEU SCORE:\n",
              "0  we had these magnificent tomatoes . in italy a...  ...    0.428264\n",
              "1  we can also act as a bridge between the people...  ...    0.404441\n",
              "2      and i am on this stage because i am a model .  ...    0.503534\n",
              "3  also i realized there was a wide gap between n...  ...    0.466584\n",
              "4  and first i commend you on your model knowledg...  ...    0.486850\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-n91DCd5rxU",
        "colab_type": "text"
      },
      "source": [
        "Plotting atention matrices for each output words with all corresspoding inputs.(Zoom in if the sentence is too long or run again to run it on a smaller sentence for better visualization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc_VdBg6EzfT",
        "colab_type": "code",
        "outputId": "ffd65ecb-1b48-4808-fe92-32aa0b5e30c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(random.choice(pairs_test)[0])\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = i want to take shipping containers and turn them into healthy cafes .\n",
            "output = va o co co la toi o co co co co co co co co toi o co co co toi o co co co co co toi o co toi o co co co co co toi o co ban co <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geem6FCT6GOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}